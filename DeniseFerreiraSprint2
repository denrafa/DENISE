import requests
import time
from typing import Tuple, Dict, List
import sqlite3


def create_table():
     dbconnection =sqlite3.connect('database.db')
     cursor = dbconnection.cursor()
     cursor.executer('''CREATE TABLE jobs(
                         id text,
                         type text,
                         url text,
                         create_at real,
                         company_url text,
                         location text,
                         title text,
                         description text,
                         how_to_apply text,
                         company_logo real
                         );''')
        
        def insert_db(cursor:sqlite3.Cursor, dbconnection=None):
            sql = '''INSERT INTO jobs(id,type,create_at,company_url,location,title,description,how_t-_apply,company_logo)
                     VALUES (123, Bachelor,12.5, www.jobsgithub.com,New York,github jobs,python,online,CPA)'''
            cursor.execute(sql,(id,type,'create_at','company_url','location','title','description','how_to_apply','company_logo'))
        
     dbconnection.commit()
     dbconnection.close()

def get_github_jobs_data() -> List[Dict]:
    """retrieve github jobs data in form of a list of dictionaries after json processing"""
    all_data = []
    page = 1
    more_data = True
    while more_data:
        url = f"https://jobs.github.com/positions.json?page={page}"
        raw_data = requests.get(url)
        if "GitHubber!" in raw_data:  # sometimes if I ask for pages too quickly I get an error; only happens in testing
            continue  # trying continue, but might want break
        partial_jobs_list = raw_data.json()
        all_data.extend(partial_jobs_list)
        if len(partial_jobs_list) < 50:
            more_data = False
        time.sleep(.1)  # short sleep between requests so I dont wear out my welcome.
        page += 1
    return all_data


def save_data(data, filename='data.txt'):
    with open(filename, 'a', encoding='utf-8') as file:
        for item in data:
            print(item, file=file)

def save_to_db(data, cursor=None):
    """:keyword data is a list of dictionaries. Each dictionary is a JSON object with a bit of jobs data"""

    cursor.execute("SELECT *FROM data")
    data =cursor.sfetchall()
    print (data)

    for item in data:
        print(data)

def main():
    data = get_github_jobs_data()
    save_data(data)
    
    
print("connection established")


if __name__ == '__main__':
    main()
